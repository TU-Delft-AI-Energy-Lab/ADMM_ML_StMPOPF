{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57db408a",
   "metadata": {
    "id": "57db408a",
    "outputId": "b8934ccc-f0bf-4326-9e57-9c317b37dd0f"
   },
   "source": [
    "# ADMM-RNN for stochastic ACOPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16bd8636",
   "metadata": {
    "id": "16bd8636"
   },
   "outputs": [],
   "source": [
    "# import all libraries needed downstream\n",
    "# import os\n",
    "# import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import torch_geometric as pyg\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# import torch_geometric.utils as utils\n",
    "import matplotlib\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import itertools\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from gurobipy import quicksum\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d5137",
   "metadata": {},
   "source": [
    "# Restoration layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4efddbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Balance_Restoration(data,nT=24,Ramp=True):\n",
    "    \n",
    "    ngen=len(data['G'])\n",
    "    nbus = len(data['Bus'])\n",
    "    Bus=data['Bus']\n",
    "    G=data['G']\n",
    "    \n",
    "    \n",
    "    #========= variables\n",
    "    Pg = cp.Variable(shape=(ngen,nT), nonneg=True, name=\"Pg\")\n",
    "\n",
    "    Obj = cp.Variable(shape=(1),nonneg=True, name=\"Obj\") \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    #========= parameters\n",
    "    Pd=cp.Parameter(shape=(nT),name='Pd')       \n",
    "    Pg_pred=cp.Parameter(shape=(ngen,nT),name='Pg_pred')        \n",
    "\n",
    "    \n",
    "    \n",
    "#     A_cost= np.multiply(np.eye(ngen,ngen), data['Gen_data'][:]['a'].to_numpy()) \n",
    "    b_cost= data['Gen_data'][:]['b'].to_numpy()\n",
    "    \n",
    "    \n",
    "       \n",
    "    \n",
    "    ## ========== Constraints\n",
    "    constraints=[]\n",
    "    \n",
    "    \n",
    "    for t in range(nT):\n",
    "#         constraints+=[data['Gen_data'][:]['Pmin'].to_numpy() <= Pg[:,t]]\n",
    "#         constraints+=[Pg[:,t]<=data['Gen_data'][:]['Pmax'].to_numpy()]\n",
    "\n",
    "\n",
    "        constraints+=[ sum(Pg[:,t]) == Pd[t] ]\n",
    "              \n",
    "        \n",
    "        if Ramp==True:\n",
    "            if t!=nT-1:\n",
    "                constraints+=[ Pg[:,t+1]-Pg[:,t]<=data['Gen_data'][:]['RampUp'].to_numpy() ]\n",
    "                constraints+=[ Pg[:,t]-Pg[:,t+1]<=data['Gen_data'][:]['RampDn'].to_numpy() ]\n",
    "                              \n",
    "                              \n",
    "    \n",
    "    Obj =  cp.sum_squares(Pg-Pg_pred) \n",
    "    objective = cp.Minimize( Obj )\n",
    "\n",
    "    problem= cp.Problem(objective, constraints)\n",
    "        \n",
    "\n",
    "            \n",
    "    return {'problem':problem,\n",
    "           'Pd':Pd, \n",
    "            'Pg':Pg,\n",
    "           'Pg_pred':Pg_pred,\n",
    "           'variables':[Pg],\n",
    "           'parameters':[Pd,Pg_pred]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ddc79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36fa6f2c",
   "metadata": {},
   "source": [
    "# Training data in correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "984666a8",
   "metadata": {
    "id": "984666a8"
   },
   "outputs": [],
   "source": [
    "\n",
    "def TrainingData_ADMM_RNN_Tseq(networkdata,dataset,Nsamples='all',seed=1):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Xd shape: [8712, ADMM_iter, Sc, T, B]\n",
    "    input: [8712, ADMM_iter, Sc, T, B+2*G]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Bus=networkdata['Bus']\n",
    "    Gen=networkdata['G']\n",
    "    Gen_data=networkdata['Gen_data']\n",
    "    DemandSet=networkdata['Demandset']\n",
    "    G2B=networkdata['G2B']\n",
    "    Bus=networkdata['Bus']\n",
    "    Pd_array=networkdata['Pd_array']\n",
    "    Nload=len(networkdata['Demandset'])\n",
    "    Nbus=len(networkdata['Bus'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Xd=dataset['Xd']; PgX=dataset['PgX']; Lambda=dataset['Lambda']; PgZ=dataset['PgZ']\n",
    "    T=Xd.shape[4]\n",
    "    #all are [8712, ADMM_iter, Sc, G/B, T]\n",
    "\n",
    "\n",
    "    print(f\"Reading dataset ... \\n Xd shape is [Samples, ADMM_iter, Sc, B, T]: {Xd.shape}\")\n",
    "\n",
    "    data_len=len(Xd)\n",
    "\n",
    "    # we find the staring instance of each sample\n",
    "    # the start can be any point with index < data_len-T\n",
    "    print(f\"{Nsamples} samples requested!\")\n",
    "    if Nsamples=='all':\n",
    "        Nsamples=data_len\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Xd=Xd.transpose((0,1,2,4,3))  #shape [8712, ADMM_iter, Sc, T, B]\n",
    "    PgX=PgX.transpose((0,1,2,4,3))\n",
    "    Lambda=Lambda.transpose((0,1,2,4,3))\n",
    "\n",
    "    plt.figure(figsize=(3,2))\n",
    "    plt.hist(Lambda.flatten(),bins=20)\n",
    "    plt.show()\n",
    "\n",
    "    # for t in range(1,T):\n",
    "    #   Lambda[:,:,:,t,:] = 0 #Lambda[:,:,:,0,:]\n",
    "\n",
    "    print('Lambda is filled with zeros for t>1')\n",
    "    print('Lambda in 24 hours', Lambda[0,0,0,:,0])\n",
    "    print('Lambda in ADMM iterations of g1',Lambda[0,:,0,0,0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    PgZ=PgZ.transpose((0,1,2,4,3))\n",
    "\n",
    "\n",
    "    input=np.concatenate((Xd,PgX,Lambda),axis=4)\n",
    "    output=PgZ\n",
    "    print('input shape: [Samples, ADMM_iter, Sc, T, B+2*G]')\n",
    "    print('input shape: ',input.shape)\n",
    "    print('output shape: ',output.shape)\n",
    "\n",
    "    print(\"[Samples, ADMM_iter, Sc] should be merged for RNN\")\n",
    "\n",
    "    #TODO! don't merge the dimensions to have test data!\n",
    "\n",
    "\n",
    "    # input=input.reshape(-1,T,len(Bus)+2*len(Gen))\n",
    "    # output=output.reshape(-1,T,len(Gen))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #shuffle the data:\n",
    "    np.random.seed(seed)\n",
    "    permutation_index = np.random.permutation(len(input))\n",
    "\n",
    "    input=input[permutation_index]\n",
    "    output=output[permutation_index]\n",
    "\n",
    "\n",
    "    return {\n",
    "        'input':input,\n",
    "        'output':output\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6HCCVwlEhYXh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6HCCVwlEhYXh",
    "outputId": "a8e5b767-0dc7-4a11-d64a-7eb36d1be397"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m-a-jUD8NxHs",
   "metadata": {
    "id": "m-a-jUD8NxHs"
   },
   "outputs": [],
   "source": [
    "# trained_model=result['model']\n",
    "\n",
    "\n",
    "\n",
    "# torch.save(result['model'].state_dict(),f'/content/gdrive/My Drive/GNN_Project/Runs_results/Model_Weights/rnn_ADMM_{system_size}bus_model_exploration_train0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c8d3a0",
   "metadata": {
    "id": "d9c8d3a0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FUpS8faqRldH",
   "metadata": {
    "id": "FUpS8faqRldH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "DMznDFf__Ooa",
   "metadata": {
    "id": "DMznDFf__Ooa"
   },
   "source": [
    "# Some Functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "wW-B9hnb0DlM",
   "metadata": {
    "id": "wW-B9hnb0DlM"
   },
   "outputs": [],
   "source": [
    "def Test_ADMMwithML(networkdata,TestDataSet,MLmodel,lamda_std=0.3,ADMM_iter=10,rho=0.1,seed=0):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Input should be TestDataSet\n",
    "\n",
    "    The input for ADMM_Stochastic_SCOPF_ML should be [Sc,Bus,T]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Bus=networkdata['Bus']\n",
    "    Gen=networkdata['G']\n",
    "\n",
    "\n",
    "    Xd = TestDataSet['Xd'][:10]                #shape [Samples, Sc, Bus, T]\n",
    "    Lambda_offline = TestDataSet['Lambda'] # [Samples, ADMM_iter, Sc, G ]\n",
    "\n",
    "    rk_offline = TestDataSet['rk']\n",
    "\n",
    "    Time_X_k_offline = TestDataSet['Time_X_k']\n",
    "    Time_Z_k_offline = TestDataSet['Time_Z_k']\n",
    "    Time_SO = TestDataSet['Time_SO']\n",
    "\n",
    "\n",
    "    PgZ_truth = TestDataSet['Pg_SO']       #[Sample,Sc,G,T]\n",
    "\n",
    "    PgX_ADMM = TestDataSet['PgX_ADMM'][:,:,:]     #shape [Samples,ADMM_iter,G,t0]\n",
    "    PgZ_ADMM = TestDataSet['PgZ_ADMM']             #shape [Samples, ADMM_iter, Sc,G,24]\n",
    "\n",
    "    print(\"Test_ADMMwithML Xd shape [Samples, Sc, Bus, T]: \",Xd.shape)\n",
    "\n",
    "\n",
    "#     print('Lambda_offline shape [Samples, ADMM_iter, Sc, G ]: ',Lambda_offline.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Nsamples=Xd.shape[0]\n",
    "\n",
    "    # ADMM_iter=10\n",
    "\n",
    "    NumSc=Xd.shape[1]\n",
    "    T=Xd.shape[3]\n",
    "    \n",
    "\n",
    "    # outputs!\n",
    "\n",
    "    PgX_ML = np.zeros((Nsamples,ADMM_iter,len(Gen)))               #[sample,ADMM_iter,G]\n",
    "\n",
    "    PgZ_ML = np.zeros((Nsamples,ADMM_iter, NumSc,len(Gen),T)) #[sample,ADMM_iter,Sc,G,T]\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    Lambda = np.random.normal(0,lamda_std,size=(Nsamples,ADMM_iter,NumSc,len(Gen)))      #[sample,ADMM_iter,Sc,G]\n",
    "\n",
    "    rk=np.zeros(shape=(Nsamples, ADMM_iter))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Time_X_k = np.zeros((Nsamples,ADMM_iter))\n",
    "    Time_Z_k = np.zeros((Nsamples,ADMM_iter))\n",
    "\n",
    "\n",
    "\n",
    "    error = np.zeros((Nsamples,ADMM_iter))\n",
    "    error0 = np.zeros((Nsamples,ADMM_iter))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    modelX = create_XUpdate_Stochastic_ACOPF(networkdata, NumSc=NumSc,nT=T,rho=rho,Ramp=True,Tlink=False)\n",
    "    \n",
    "\n",
    "\n",
    "    for i in tqdm(range(Nsamples)):\n",
    "#     for i in tqdm(range(5)):\n",
    "\n",
    "\n",
    "        res=ADMM_Stochastic_ACOPF_ML(data=networkdata,MLmodel=MLmodel, modelX=modelX,\n",
    "                                        All_Sc=True,lambda_initial=Lambda[i,0,:,:],\n",
    "                                        Ramp=True,DemandInstnace=Xd[i,:,:,:] , #demand instance is [Sc,Bus,T]\n",
    "                                   k_iter=ADMM_iter,rho=rho,print_result=False)\n",
    "\n",
    "        PgZ_ML[i]=res['PgZ_k']   #shape [ADMM_iter,SC,G,T]\n",
    "        PgX_ML[i]=res['PgX_k'][:,:,0]        #shape [ADMM_iter,G,t0]\n",
    "        Lambda[i] = res['lambda_k']\n",
    "        rk[i]=res['rk']\n",
    "        # print(PgX.shape)\n",
    "\n",
    "        Time_X_k[i] = res['Time_X_k']\n",
    "        Time_Z_k[i] = res['Time_Z_k']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(error.shape[0]):\n",
    "        for k in range(error.shape[1]):\n",
    "            error0[i,k] = np.square(np.absolute(PgX_ML[i,k,:] - PgZ_truth[i,0,:,0])).sum()/PgZ_truth[i,0,:,0].size\n",
    "            error[i,k] = np.square(np.absolute(PgZ_ML[i,k,:,:,:] - PgZ_truth[i,:,:,:])).sum().sum().sum()/PgZ_truth[i,:,:,:].size\n",
    "\n",
    "\n",
    "\n",
    "    error_offline = np.zeros((Nsamples,PgZ_ADMM.shape[1]))\n",
    "    error0_offline = np.zeros((Nsamples,PgZ_ADMM.shape[1]))\n",
    "\n",
    "\n",
    "    for i in range(error_offline.shape[0]):\n",
    "        for k in range(error_offline.shape[1]):\n",
    "            error0_offline[i,k] = np.square(np.absolute(PgX_ADMM[i,k,:] - PgZ_truth[i,0,:,0])).sum()/PgZ_truth[i,0,:,0].size\n",
    "            error_offline[i,k] = np.square(np.absolute(PgZ_ADMM[i,k,:,:,:] - PgZ_truth[i,:,:,:])).sum().sum().sum()/PgZ_truth[i,:,:,:].size \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #=========================== accuracy error\n",
    "    plt.figure(figsize=(4,2.5))\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 2.5))\n",
    "\n",
    "    y_min = min(error_offline.mean(axis=0))\n",
    "    y_max = 1.0#max(error0.mean(axis=0))\n",
    "\n",
    "    ax2.set_title('ML Eror vs truth')\n",
    "    ax2.plot(error.mean(axis=0),label='error')\n",
    "    ax2.plot(error0.mean(axis=0),label='error0')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_ylim(y_min, y_max)\n",
    "    ax2.legend()\n",
    "\n",
    "    ax1.set_title('ADMM Eror vs truth')\n",
    "    ax1.plot(error_offline.mean(axis=0),label='error')\n",
    "    ax1.plot(error0_offline.mean(axis=0),label='error0')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_ylim(y_min, y_max)\n",
    "    ax1.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    #=========================== rk\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 2.5))\n",
    "    ax1.plot(rk_offline[:,:ADMM_iter].mean(axis=0),label='rk_offline')\n",
    "    ax1.plot(rk.mean(axis=0),label='rk_ML')\n",
    "    ax1.legend()\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_title('rk test data')\n",
    "\n",
    "    ax2.plot(rk.mean(axis=0),label='rk ML')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_title('rk ML ')\n",
    "    plt.show()\n",
    "\n",
    "    #=========================== lambda distribution\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 2.5))\n",
    "\n",
    "    ax2.set_title('Lambda ML')\n",
    "    ax2.hist(Lambda[:,:,:,0].flatten(),bins=20)\n",
    "    ax1.set_title('Lambda ADMM offline')\n",
    "    ax1.hist(Lambda_offline[:,:,:,0].flatten(),bins=20)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        'Xd':Xd,\n",
    "        'Lambda_ML':Lambda,\n",
    "        'PgX_ML':PgX_ML,\n",
    "        'PgZ_ML':PgZ_ML,\n",
    "\n",
    "        'error0_ML':error0,\n",
    "        'error_ML':error,\n",
    "        'error0_offline':error0_offline,\n",
    "        'error_offline':error_offline,\n",
    "\n",
    "        'PgZ_truth':PgZ_truth,\n",
    "\n",
    "        'PgX_ADMM_offline':PgX_ADMM,\n",
    "        'PgZ_ADMM_offline':PgZ_ADMM,\n",
    "        'Lambda_offline':Lambda_offline,\n",
    "        'rk_ML':rk,\n",
    "        'rk_offline':rk_offline,\n",
    "\n",
    "        'Time_X_k_ML':Time_X_k,\n",
    "        'Time_Z_k_ML':Time_Z_k,\n",
    "        'Time_X_k_offline':Time_X_k_offline,\n",
    "        'Time_Z_k_offline':Time_Z_k_offline,\n",
    "        'Time_SO':Time_SO}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DC5F56LFjQ6m",
   "metadata": {
    "id": "DC5F56LFjQ6m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "v4F7HQkQ9lWW",
   "metadata": {
    "id": "v4F7HQkQ9lWW"
   },
   "outputs": [],
   "source": [
    "#=========================\n",
    "def ADMM_Stochastic_ACOPF_ML(data,MLmodel,\n",
    "                           modelX=None,k_iter=10,All_Sc=True,\n",
    "                        rho=10,rho_decay=3e-1,rho_min=1.0,lambda_initial=None,\n",
    "                        Ramp=True,DemandInstnace=None,Pg0=None,print_result=False):\n",
    "\n",
    "     #======  data\n",
    "\n",
    "    Sbase=data['Sbase']\n",
    "    Bus=data['Bus']    # for b in Bus\n",
    "    branch=data['branch']\n",
    "    Lines=data['Lines']\n",
    "    L2B=data['L2B']\n",
    "    Pdemand=data['Pdemand']\n",
    "    DemandSet=data['Demandset']     # for d in Demand_set\n",
    "    D2B=data['D2B']                  # for d,b in D2B\n",
    "    Gen_data=data['Gen_data']\n",
    "    G=data['G']             #for g in Gset\n",
    "    G2B=data['G2B']               #for g,b in G2B\n",
    "#     EndTime=data['EndTime']\n",
    "    Pd_array=data['Pd_array']\n",
    "    Qd_array=data['Qd_array']\n",
    "\n",
    "\n",
    "    #======\n",
    "\n",
    "\n",
    "    # change here!\n",
    "    if DemandInstnace is not None:\n",
    "        Pd_sc_array=DemandInstnace.copy() #shape [Sc,Bus,T]\n",
    "        EndTime=Pd_sc_array.shape[2]-1\n",
    "        T=range(0,EndTime+1) #0 to 23\n",
    "\n",
    "        NumSc=Pd_sc_array.shape[0]\n",
    "        Senarios=range(0,NumSc)\n",
    "    else:\n",
    "        print('No demand input!!')\n",
    "\n",
    "\n",
    "\n",
    "    k_iter_list=range(k_iter)\n",
    "    residual={'rk':[],'sk':[]}\n",
    "\n",
    "\n",
    "    Time_X_k=np.array([]) #(k,)\n",
    "    Time_Z_k=np.array([]) #(k,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## ======================== Initialization =========================================\n",
    "\n",
    "\n",
    "#     Multi_OPF() for average scenarios\n",
    "\n",
    "#     PgX[g]\n",
    "#     PgZ[sc,g,t] => we pass PgZ[g,0,sc] to X-Update\n",
    "\n",
    "    PgX=np.zeros((len(G)))\n",
    "    PgZ=np.zeros((NumSc,len(G),len(T))) # => we pass PgZ[:,g,0] to X-Update\n",
    "\n",
    "    PgX_k=np.zeros(( k_iter,len(G),len(T))) #shape [ADMM_iter,G,T] we do MP-OPF for better convergence\n",
    "    PgZ_k = np.zeros((k_iter,NumSc,len(G),len(T))) # => shape [ADMM_iter,Sc,G,T]\n",
    "\n",
    "    if lambda_initial is None:\n",
    "        lambda_s=np.random.normal(0,0.1,size=(NumSc,len(G)))\n",
    "    else:\n",
    "        lambda_s = lambda_initial\n",
    "\n",
    "#     lambda_s=np.zeros((NumSc,len(G)))\n",
    "    lambda_k=np.zeros((k_iter,NumSc,len(G))) #shape [ADMM_iter,Sc,G]\n",
    "\n",
    "\n",
    "\n",
    "    if modelX is None:\n",
    "        modelX = create_XUpdate_Stochastic_ACOPF(data, NumSc=NumSc,nT=T,rho=rho,Ramp=True,Tlink=False)\n",
    "    \n",
    "\n",
    "    #initialization\n",
    "    result = Solve_Xupdate_ACOPF(data,modelX,Pd_sc_array,PgZ[:,:,0],lambda_s)\n",
    "    \n",
    "#     result=Solve_XUpdate_Stochastic_multi_SCOPF(data,modelX=modelX, cont_list=cont_list,\n",
    "#                                                 PgZ_s=PgZ[:,:,0],lambda_s=lambda_s,rho=rho,\n",
    "#                                         Ramp=Ramp,DemandInstnace=Pd_sc_array.mean(axis=0),Pg0=Pg0)\n",
    "\n",
    "\n",
    "    PgX=result['Pg'][:,0]\n",
    "    PgZ[:,:,:]=result['Pg']\n",
    "\n",
    "\n",
    "\n",
    "    ## ======================== ADMM loop =========================================\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for k in range(k_iter):\n",
    "\n",
    "#         print('\\n===== ADMM iteration ',k)\n",
    "\n",
    "    #============= X-Update ==============================================\n",
    "\n",
    "        #save lambda as input!\n",
    "        lambda_k[k] = lambda_s\n",
    "\n",
    "        result = Solve_Xupdate_ACOPF(data,modelX,Pd_sc_array,PgZ[:,:,0],lambda_s)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         Solve_XUpdate_Stochastic_multi_SCOPF(data=data,modelX=modelX,cont_list=cont_list,    #XUpdate_Stochastic_OPF()\n",
    "#                                     PgZ_s=PgZ[:,:,0],lambda_s=lambda_s,\n",
    "#                                       rho=rho,Ramp=Ramp,\n",
    "#                                       DemandInstnace=Pd_sc_array.mean(axis=0),  #Pd_sc_array[0,:,0]\n",
    "#                                       Pg0=Pg0,print_result=False)\n",
    "\n",
    "        PgX=result['Pg'][:,0] #we take the first value\n",
    "        PgXt=result['Pg'] #input for ML model\n",
    "        PgX_k[k]=result['Pg']\n",
    "\n",
    "        ex_time=result['time']\n",
    "        Time_X_k = np.concatenate((Time_X_k,np.array([ex_time])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #============= Z-Update ==============================================\n",
    "        if All_Sc==False:\n",
    "            for sc in Senarios:\n",
    "                if print_result==True:\n",
    "                    print('This option is not used in Ml model')\n",
    "\n",
    "\n",
    "        elif All_Sc==True:\n",
    "#             if print_result==True:\n",
    "#                     print('===== iteration %2s'%(k),end='\\r')\n",
    "\n",
    "            #TODO ML here\n",
    "            # ML input is [batch,T,B+2G]\n",
    "            # ML output is [batch,T,G], PgZ is [Sc,G,T]\n",
    "            # Pd_sc_array is [Sc,Bus,T] -> [SC,T,B]\n",
    "            # lambda_s is [Sc,G]  -> [Sc,T,G]\n",
    "            # PgX is [G,T] -> [Sc,T,G]\n",
    "\n",
    "            MLmodel.to(device)\n",
    "            MLmodel.eval()     #evaluation mode\n",
    "\n",
    "\n",
    "\n",
    "            Pd_input = Pd_sc_array.transpose((0,2,1))  # Pd_sc_array is [Sc,Bus,T] -> [SC,T,B]\n",
    "            # print(\"Pd_input: \",Pd_input.shape)\n",
    "\n",
    "            lambda_input = np.zeros((NumSc,len(T),len(G))) # lambda_s is [Sc,G]  -> [Sc,T,G]\n",
    "\n",
    "            lambda_input[:,0,:] = lambda_s #other times are zeros!\n",
    "            for t in range(1,len(T)):\n",
    "              lambda_input[:,t,:] = lambda_input[:,0,:] #repeat itself\n",
    "\n",
    "\n",
    "            PgX_input = np.zeros((NumSc,len(T),len(G))) # PgX is [G,T] -> [Sc,T,G]\n",
    "            for sc in range(NumSc):\n",
    "                PgX_input[sc,:,:] = PgXt.T\n",
    "\n",
    "\n",
    "\n",
    "            inputt = np.concatenate( (Pd_input,PgX_input,lambda_input),axis=2)\n",
    "\n",
    "            inputt = torch.Tensor(inputt).to(device)\n",
    "\n",
    "            # print(\"input.shape: \",input.shape)\n",
    "\n",
    "\n",
    "            # ML prediction!\n",
    "            st=time.time()\n",
    "            PgZ_pred = MLmodel(inputt)[0].detach()\n",
    "            et=time.time()\n",
    "\n",
    "            ex_time=et-st\n",
    "            Time_Z_k = np.concatenate((Time_Z_k,np.array([ex_time])))\n",
    "\n",
    "            # print(\"PgZ_pred shape:\", PgZ_pred.shape)\n",
    "\n",
    "\n",
    "            PgZ=np.array(PgZ_pred.cpu()).transpose((0,2,1)) # [Sc,G,T]\n",
    "\n",
    "            # dual update\n",
    "            for g_ind,g in enumerate(G):\n",
    "                for sc_ind,sc in enumerate(Senarios):\n",
    "                    lambda_s[sc_ind,g_ind]+=rho*(PgX[g_ind]-PgZ[sc_ind,g_ind,0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # end of z-update! ===============================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #============= Residuals ==============================================\n",
    "\n",
    "        # k_update\n",
    "        PgZ_k[k]=PgZ\n",
    "#         lambda_k[k]=lambda_s\n",
    "\n",
    "        rk_now=np.square(np.absolute(PgX - PgZ[:,:,0])).mean()\n",
    "\n",
    "        residual['rk']+=[rk_now]\n",
    "\n",
    "        # residual['rk']+= mean_squared_error(np.repeat(PgX,len(Senarios)).reshape(len(Senarios),-1)  ,PgZ[:,:,0])\n",
    "\n",
    "        rho=np.maximum(rho-rho_decay,rho_min)\n",
    "        # print('\\n lambda[sc,g1] is: ',lambda_s[:,0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    ex_time=end_time-start_time  #execution time\n",
    "\n",
    "\n",
    "    if print_result==True:\n",
    "        print(\"\\n******* finished!!! ********\")\n",
    "        print(\"ADMM time: %.3f sec\"%ex_time)\n",
    "\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(5,3))\n",
    "\n",
    "        plt.plot(range(k_iter), residual['rk']   )\n",
    "\n",
    "        plt.title('Residuals',fontsize=15)\n",
    "        plt.xlabel('Iteration k',fontsize=12)\n",
    "        plt.yscale('log')\n",
    "    #     plt.grid(ls='--')\n",
    "        plt.show('Plot1')\n",
    "\n",
    "\n",
    "\n",
    "    #     plt.grid(ls='--')\n",
    "        plt.figure(figsize=(5,3))\n",
    "        for sc in Senarios:\n",
    "            plt.plot(PgZ[sc,0,:],label=sc)\n",
    "        plt.plot(PgZ[:,0,:].mean(axis=0), linestyle='dashed' , linewidth=2, color='black' ,label='mean')\n",
    "        # plt.legend()\n",
    "        plt.title('G1 for sc')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        'PgZ':PgZ, #[Sc,G,T]\n",
    "        'PgX':PgX,\n",
    "        'PgX_k':PgX_k, #shape [ADMM_iter,G,T]\n",
    "        'PgZ_k':PgZ_k, #shape [ADMM_iter,Sc,G,T]\n",
    "        'lambda_k':lambda_k, #shape [ADMM_iter,Sc,G]\n",
    "        'rk':residual['rk'], #[ADMM_iter],\n",
    "        'Time_X_k':Time_X_k,\n",
    "        'Time_Z_k':Time_Z_k\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4297cf19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6d915de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_XUpdate_Stochastic_ACOPF(data,nT=24,NumSc=10,rho=1,   \n",
    "                           Ramp=True,Tlink=False):\n",
    "    \"\"\"This function is used for single case studies and execution time.\"\"\"\n",
    "    \n",
    "    #======  data\n",
    "    \n",
    "    Sbase=data['Sbase']\n",
    "    Bus=data['Bus']    # for b in Bus\n",
    "    branch=data['branch']\n",
    "    Lines=data['Lines']\n",
    "    L2B=data['L2B']\n",
    "    Pdemand=data['Pdemand']\n",
    "    DemandSet=data['Demandset']     # for d in Demand_set\n",
    "    D2B=data['D2B']                  # for d,b in D2B\n",
    "    Gen_data=data['Gen_data']\n",
    "    G=data['G']             #for g in Gset\n",
    "    G2B=data['G2B']               #for g,b in G2B\n",
    "#     EndTime=data['EndTime']\n",
    "    Pd_array=data['Pd_array']\n",
    "    Qd_array=data['Qd_array']\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    #======\n",
    "        \n",
    "    Senarios=range(0,NumSc)\n",
    "    T=range(nT)\n",
    "    EndTime=nT-1\n",
    "        \n",
    "    \n",
    "    #====\n",
    "    \n",
    "    model =  ConcreteModel(name='ACOPF')\n",
    "\n",
    "\n",
    "    model.Pg = Var( G,T,bounds=(0, None) )\n",
    "    model.Qg = Var( G,T,bounds=(None, None) )\n",
    "    \n",
    "    \n",
    "    model.lambdaP = Param(Senarios,G,initialize=0, mutable=True)\n",
    "    model.PgZ = Param(Senarios,G,initialize=0, mutable=True)\n",
    "    \n",
    "#     model.lambdaQ = Param(Bus,T,initialize=0, mutable=True)\n",
    "    \n",
    "    model.Pd = Param(Bus,T,initialize=0, mutable=True)\n",
    "    model.Qd = Param(Bus,T,initialize=0, mutable=True)\n",
    "    if Tlink==True:\n",
    "        model.Pg0 = Param(G,mutable=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    model.V2 = Var(Bus,T,bounds=(0, None),initialize=1 )\n",
    "    model.L2 = Var(Lines,T,bounds=(0, None) )\n",
    "\n",
    "    model.Pflow = Var( Lines,T,bounds=(None, None) )\n",
    "    model.Qflow = Var( Lines,T,bounds=(None, None) )\n",
    "\n",
    "\n",
    "    model.OF = Var( bounds=(0, None) )\n",
    "\n",
    "    #initialize\n",
    "#     if Pg0 != None:\n",
    "#         for g in G:\n",
    "#             model.Pg0[g].value=Pg0[g]\n",
    "#             for t in T:\n",
    "#                 model.Pg[g,t].value=Pg0[g]\n",
    "\n",
    "\n",
    "\n",
    "    # equations\n",
    "    vmin=0.9; vmax=1.1; \n",
    "\n",
    "\n",
    "\n",
    "    def eqPbalance(model,b,t):\n",
    "        return sum( model.Pg[g,t] for g,b in G2B.select('*',b) ) - model.Pd[b,t] \\\n",
    "        == sum(model.Pflow[l,i,j,t] for l,i,j in Lines.select('*',b,'*'))\\\n",
    "    -sum(model.Pflow[l,i,j,t]-branch.loc[(l,i,j)]['r']*model.L2[l,i,j,t]         for l,i,j in Lines.select('*','*',b))\n",
    "    model.eqPbalance=Constraint(Bus,T,rule=eqPbalance)\n",
    "\n",
    "\n",
    "    def eqQbalance(model,b,t):\n",
    "        return sum( model.Qg[g,t] for g,b in G2B.select('*',b) ) - model.Qd[b,t] \\\n",
    "        == sum(model.Qflow[l,i,j,t]      for l,i,j in Lines.select('*',b,'*'))\\\n",
    "    -sum(model.Qflow[l,i,j,t]-branch.loc[(l,i,j)]['x']*model.L2[l,i,j,t]    for l,i,j in Lines.select('*','*',b))\\\n",
    "    #+0.5*branch.loc[(l,i,j)]['b_ij']*model.V2[b,t]\n",
    "    \n",
    "    model.eqQbalance=Constraint(Bus,T,rule=eqQbalance)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def eqSij(model,l,i,j,t):\n",
    "        return model.Pflow[l,i,j,t]**2+model.Qflow[l,i,j,t]**2 <= model.V2[i,t]*model.L2[l,i,j,t] \n",
    "    model.eqSij=Constraint(Lines,T,rule=eqSij)\n",
    "\n",
    "    #\n",
    "    def eqSij_V(model,l,i,j,t):\n",
    "        return model.V2[i,t]-model.V2[j,t] ==\\\n",
    "    -branch.loc[(l,i,j)]['z2']*model.L2[l,i,j,t]\\\n",
    "    +2*(branch.loc[(l,i,j)]['r']*model.Pflow[l,i,j,t] + branch.loc[(l,i,j)]['x']*model.Qflow[l,i,j,t] )\n",
    "    model.eqSij_V=Constraint(Lines,T,rule=eqSij_V)\n",
    "\n",
    "\n",
    "\n",
    "#     def eqflow2(model,l,i,j,t):\n",
    "#             return model.Pflow[l,i,j,t] + model.Pflow[l,j,i,t] >= 0\n",
    "#     model.eqflow2=Constraint(Lines,T,rule=eqflow2)\n",
    "\n",
    "\n",
    "    def eqVmax(model,i,t): return model.V2[i,t]<= vmax**2\n",
    "    model.eqVmax=Constraint(Bus,T,rule=eqVmax)\n",
    "\n",
    "    def eqVmin(model,i,t): return vmin**2 <= model.V2[i,t]\n",
    "    model.eqVmin=Constraint(Bus,T,rule=eqVmin)\n",
    "\n",
    "\n",
    "    def eqPijmax(model,l,i,j,t): \n",
    "        return model.Pflow[l,i,j,t]*model.Pflow[l,i,j,t] + model.Qflow[l,i,j,t]*model.Qflow[l,i,j,t]\\\n",
    "        <=branch.loc[(l,i,j)]['limit']**2\n",
    "    model.eqPijmax=Constraint(Lines,T,rule=eqPijmax)\n",
    "\n",
    "    def eqPijmin(model,l,i,j,t): \n",
    "        return -branch.loc[(l,i,j)]['limit']**2 <=\\\n",
    "        model.Pflow[l,i,j,t]*model.Pflow[l,i,j,t] + model.Qflow[l,i,j,t]*model.Qflow[l,i,j,t]\n",
    "    model.eqPijmin=Constraint(Lines,T,rule=eqPijmin)\n",
    "\n",
    "\n",
    "\n",
    "    def eqPgmax(model,g,t): return model.Pg[g,t]<=Gen_data.loc[g]['Pmax']\n",
    "    model.eqPgmax=Constraint(G,T,rule=eqPgmax)\n",
    "\n",
    "    def eqPgmin(model,g,t): return Gen_data.loc[g]['Pmin'] <= model.Pg[g,t]\n",
    "    model.eqPgmin=Constraint(G,T,rule=eqPgmin)\n",
    "\n",
    "    def eqQgmax(model,g,t): return model.Qg[g,t]<= Gen_data.loc[g]['Qmax']\n",
    "    model.eqQgmax=Constraint(G,T,rule=eqQgmax)\n",
    "\n",
    "    def eqQgmin(model,g,t): return Gen_data.loc[g]['Qmin'] <= model.Qg[g,t]\n",
    "    model.eqQgmin=Constraint(G,T,rule=eqQgmin)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    if Ramp==True:\n",
    "        def eqRU(model,g,t):\n",
    "            if t!=EndTime:\n",
    "                return model.Pg[g,t+1]-model.Pg[g,t]<=Gen_data.loc[g]['RampUp']  #+model.Rup[g,t+1]\n",
    "            else:\n",
    "                return Constraint.Skip\n",
    "        model.eqRU=Constraint(G,T,rule=eqRU)\n",
    "        \n",
    "        def eqRD(model,g,t):\n",
    "            if t!=EndTime:\n",
    "                return model.Pg[g,t]-model.Pg[g,t+1]<=Gen_data.loc[g]['RampDn']  \n",
    "            else:\n",
    "                return Constraint.Skip\n",
    "        model.eqRD=Constraint(G,T,rule=eqRD)\n",
    "        \n",
    "        if Tlink==True:\n",
    "            def eqPg0up(model,g):\n",
    "                return model.Pg[g,T[0]]-model.Pg0[g]<=Gen_data.loc[g]['RampUp'] \n",
    "            model.eqPg0up=Constraint(G,rule=eqPg0up)\n",
    "            \n",
    "            def eqPg0dn(model,g):\n",
    "                return model.Pg0[g] - model.Pg[g,T[0]]<=Gen_data.loc[g]['RampDn'] \n",
    "            model.eqPg0dn=Constraint(G,rule=eqPg0dn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model.eqOF = Constraint( expr = model.OF == sum(Gen_data.loc[g]['b']*model.Pg[g,t]\n",
    "                                                #+Gen_data.loc[g]['a']*model.Pg[g,t]*model.Pg[g,t]\n",
    "                                                 + 0.0001*Gen_data.loc[g]['b']*model.Qg[g,t]*model.Qg[g,t] \n",
    "                                                    for g in G for t in T)\n",
    "                    +sum( model.lambdaP[sc,g]*(model.Pg[g,0]-model.PgZ[sc,g]) \n",
    "                         + 0.5*rho*(model.Pg[g,0]-model.PgZ[sc,g])**2    for g in G for sc in Senarios) )\n",
    "    \n",
    "    \n",
    "    model.obj = Objective( expr = model.OF , sense=pyo.minimize      )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5a524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "887b770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Solve_Xupdate_ACOPF(data,modelX,Pd_sc_array,PgZ,lambda_s):\n",
    "    \n",
    "    \n",
    "    Bus=data['Bus']\n",
    "    G=data['G']\n",
    "    Pd_array=data['Pd_array']\n",
    "    Qd_array=data['Qd_array']\n",
    "    \n",
    "    nT=Pd_sc_array.shape[2]\n",
    "    NumSc=Pd_sc_array.shape[0]\n",
    "    \n",
    "    \n",
    "    solver = SolverFactory('ipopt')\n",
    "    \n",
    "    Qd_sc_array=Pd_sc_array.copy()\n",
    "        \n",
    "    for b_ind in range(len(Bus)):\n",
    "        if Pd_array[b_ind,0]!=0:\n",
    "            Qd_sc_array[:,b_ind,:] = Pd_sc_array[:,b_ind,:]*(Qd_array[b_ind,0]/Pd_array[b_ind,0])\n",
    "\n",
    "    \n",
    "    \n",
    "    for b_ind,b in enumerate(Bus):\n",
    "        for t in range(nT):\n",
    "            modelX.Pd[b,t].value=Pd_sc_array[:,b_ind,t].mean()\n",
    "            modelX.Qd[b,t].value=Qd_sc_array[:,b_ind,t].mean()    \n",
    "    \n",
    "\n",
    "    for sc in range(NumSc):\n",
    "        for g_ind,g in enumerate(G):\n",
    "            modelX.PgZ[sc,g].value = PgZ[sc,g_ind]\n",
    "            modelX.lambdaP[sc,g].value = lambda_s[sc,g_ind]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    results = solver.solve(modelX,tee=False) \n",
    "    \n",
    "    ex_time = results['Solver'][0]['Time']\n",
    "    \n",
    "    \n",
    "    Pgt_array=np.zeros((len(G),nT))\n",
    "    for ind,g in enumerate(G):\n",
    "        for t in range(nT):\n",
    "            Pgt_array[ind,t]=modelX.Pg[g,t].value\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'Pg':Pgt_array,\n",
    "        'time':ex_time\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SMKOkT1x0BmS",
   "metadata": {
    "id": "SMKOkT1x0BmS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bex7-W4w5PIU",
   "metadata": {
    "id": "bex7-W4w5PIU"
   },
   "outputs": [],
   "source": [
    "def run_casestudy(data_x_bus,dataset,T=6,seed=1,epochs=100,trained_model=None):\n",
    "\n",
    "    Bus=data_x_bus['Bus']\n",
    "    Gen=data_x_bus['G']\n",
    "\n",
    "\n",
    "    data=TrainingData_ADMM_RNN_Tseq(data_x_bus,dataset,Nsamples='all')\n",
    "\n",
    "    Full_x, Full_y = torch.Tensor(data['input']), torch.Tensor(data['output'])\n",
    "    del data\n",
    "    print('input imported and deleted!')\n",
    "\n",
    "    # o_f = Full_y.shape[4]\n",
    "\n",
    "\n",
    "    # create sizes of training, testing and validation data, 80% as training data, 20% validation\n",
    "    train_size = int((Full_x.shape[0]//4) * 3)\n",
    "    val_size = int((Full_x.shape[0]//4) )\n",
    "    # test_size = int((Full_x.shape[0]//3) / 2)\n",
    "\n",
    "    # divide X features into training, testing and validation respectively\n",
    "    train_x = Full_x[:train_size,:,:].reshape(-1,T,len(Bus)+2*len(Gen))\n",
    "    val_x = Full_x[train_size:train_size+val_size,:,:].reshape(-1,T,len(Bus)+2*len(Gen))\n",
    "    # test_x = Full_x[train_size+val_size:,:,:].reshape(-1,T,len(Bus)+2*len(Gen))\n",
    "    del Full_x\n",
    "\n",
    "\n",
    "    # divide Y output into training, testing and validation respectively\n",
    "    train_y = Full_y[:train_size,:,:].reshape(-1,T,len(Gen))\n",
    "    val_y = Full_y[train_size:train_size+val_size,:,:].reshape(-1,T,len(Gen))\n",
    "    # test_y = Full_y[train_size+val_size:,:,:].reshape(-1,T,len(Gen))\n",
    "    del Full_y\n",
    "\n",
    "    print(\"train_x.shape: \",train_x.shape)\n",
    "    print(\"train_y.shape: \",train_y.shape)\n",
    "\n",
    "    # create training, testing and validation data with Tensor dataset\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    val_dataset = TensorDataset(val_x, val_y)\n",
    "\n",
    "\n",
    "    num_samples = train_x.shape[0]\n",
    "    seq_length = train_x.shape[1]\n",
    "    input_dim = train_x.shape[2]\n",
    "    output_dim = train_y.shape[2]\n",
    "\n",
    "    del train_x, train_y\n",
    "    del val_x, val_y\n",
    "    # test_dataset = TensorDataset(test_x, test_y)\n",
    "\n",
    "    batch_size= 16\n",
    "\n",
    "    # create iterable data loader object for training, validation and testing data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    num_layers = 3\n",
    "    # hidden_feat\n",
    "    # learning_rate = 0.001\n",
    "\n",
    "\n",
    "\n",
    "    # instantiate the RNN class object with the parameters above\n",
    "    torch.manual_seed(seed)\n",
    "    rnn_model = RNN(input_dim, hidden_feat, output_dim,num_layers,batch_size)\n",
    "    print(rnn_model)\n",
    "\n",
    "    # count the number of parameters in this model\n",
    "    tot_params = 0\n",
    "    for parameter in rnn_model.parameters():\n",
    "        layer_ws = 1\n",
    "        for val in parameter.shape:\n",
    "            layer_ws*=val\n",
    "        tot_params += layer_ws\n",
    "    print(f\"Total number of parameters = {tot_params}\")\n",
    "\n",
    "\n",
    "  # param_values = create_hyper_combination()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # =========================================== Training ============================================================================\n",
    "    if trained_model is not None:\n",
    "        rnn_model.load_state_dict(torch.load(trained_model))\n",
    "        print('\\n\\n loading trained model weights! \\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    start=time.time()\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    del train_dataset, val_dataset\n",
    "\n",
    "    optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.0001, weight_decay=5e-5)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    for epoch in range(epochs+1):\n",
    "        print('epoch: ',epoch)\n",
    "        train_loss = train_rnn(rnn_model, train_loader, optimizer)\n",
    "        valid_loss = evaluate_rnn(rnn_model, val_loader)\n",
    "        training_losses.append(train_loss)\n",
    "        validation_losses.append(valid_loss)\n",
    "        # wandb.log({\"training_loss\": train_loss, \"validation_loss\": valid_loss})\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch: {epoch}')\n",
    "            print(f'\\tTrain Loss: {train_loss:.4f}')\n",
    "            print(f'\\t Val. Loss: {valid_loss:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "    end=time.time()\n",
    "    #plt.style.use('seaborn-darkgrid')\n",
    "    plt.subplots(figsize=(5,3))\n",
    "    plt.plot([i for i in range(len(training_losses))], training_losses, 'r', label='Training loss', linestyle='solid')\n",
    "    plt.plot([i for i in range(len(validation_losses))], validation_losses, 'g', label='Validation loss', linestyle='dashed')\n",
    "    plt.legend()\n",
    "    plt.title(f'RNN Training and Validation loss for {system_size} Bus system',fontsize = 15)\n",
    "    plt.xlabel('Epochs',fontsize = 12)\n",
    "    plt.ylabel('MSE Loss',fontsize = 12)\n",
    "    #plt.savefig(f'/content/drive/MyDrive/Colab Notebooks/DCOPF_experiment_logs/RNN/RNN_training_and_validation_loss_{system_size}_bus.png',dpi=1200)\n",
    "    plt.show()\n",
    "    print('Training time is: %.2f'%(end-start))\n",
    "\n",
    "    training_losses=np.array(training_losses)\n",
    "    validation_losses=np.array(validation_losses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        'training_losses':training_losses,\n",
    "        'validation_losses':validation_losses,\n",
    "        'model':rnn_model\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9WhVhgd30CJ0",
   "metadata": {
    "id": "9WhVhgd30CJ0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2f5f691e",
   "metadata": {
    "id": "2f5f691e"
   },
   "outputs": [],
   "source": [
    "# This is the RNN class\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,batch_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size=batch_size\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.pmax = torch.Tensor(data_x_bus['Gen_data']['Pmax'].values).to(device)\n",
    "        \n",
    "        self.NT = 24\n",
    "        self.NB = len(data_x_bus['Bus'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.to(device)\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out)\n",
    "        out = F.sigmoid(out).to(device)\n",
    "\n",
    "        out = out * self.pmax\n",
    "        \n",
    "        Pg_pred = torch.transpose(out,1,2).to(device) #[batch,G,T]\n",
    "        xd = torch.transpose(x,1,2)[:,:self.NB,:].sum(axis=1).to(device) #[batch,Bus,T]=>[batch,T]\n",
    "        \n",
    "        Pg_r, = layer_balance(xd,Pg_pred) #[batch,G,T]\n",
    "        \n",
    "        \n",
    "        \n",
    "        Pg_r =  torch.transpose(Pg_r,1,2) #[batch,T,G] \n",
    "        Pg_pred = torch.transpose(Pg_pred,1,2) #[batch,T,G] \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        return Pg_r,Pg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6205d",
   "metadata": {
    "id": "9eb6205d"
   },
   "outputs": [],
   "source": [
    "# function for model training , returns loss per epoch after summing loss per batch and dividing by number of batches in data loader object\n",
    "def train_rnn(model, loader,optimizer,device = device):\n",
    "\n",
    "    NG = len(data_x_bus['G'])\n",
    "    ND = system_size\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for inputs, targets in tqdm(loader):\n",
    "\n",
    "        inputs = inputs.to(device)  #shape: [batch,T, D+PgX+Lambda]\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # batch_size = inputs.size(0)\n",
    "\n",
    "        outputs,Pg_pred = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets) + 0.5*criterion(outputs, Pg_pred)\n",
    "\n",
    "\n",
    "        loss_t0 = criterion(outputs[:,0,:], targets[:,0,:]) #t0 loss is more important\n",
    "\n",
    "\n",
    "        # ADMM loss t0\n",
    "        PgX = inputs[:,0,ND:ND+NG].detach()      #[batch,t0,G] D,G,G\n",
    "        lambda_s = inputs[:,0,ND+NG:].detach() #[batch,t0,G]\n",
    "        rho = 0.1\n",
    "\n",
    "        ADMM_loss = 1e-2 * lambda_s * (outputs[:,0,:]-PgX) + 1e-1 *(rho/2)*torch.square(  outputs[:,0,:]-PgX  )\n",
    "        ADMM_loss = ADMM_loss.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        loss = loss + 10*loss_t0 + 1*ADMM_loss  #you can change and adjust the loss function for better results. This is a heuristic here\n",
    "\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc6702d5",
   "metadata": {
    "id": "bc6702d5"
   },
   "outputs": [],
   "source": [
    "# function for model validation , returns loss per epoch after summing loss per batch and dividing by number of batches in data loader object\n",
    "def evaluate_rnn(model, loader, device=device):\n",
    "    model.to(device)\n",
    "    model.eval() # specifies that the model is in evaluation mode\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            outputs = model(inputs)[0]\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        return epoch_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "v4vi334lG2dv",
   "metadata": {
    "id": "v4vi334lG2dv"
   },
   "outputs": [],
   "source": [
    "# function to run trained model through test data, save model prediction per batch and true generator outputs to list\n",
    "def test_rnn(model, loader, device=device):\n",
    "    model.to(device)\n",
    "    model.eval() # specifies that the model is in evaluation mode\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model_preds = []\n",
    "    actual_y = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            outputs = model(inputs)[0]\n",
    "            model_preds.append(outputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            actual_y.append(targets)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        return epoch_loss / len(loader), model_preds, actual_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0xnDmcBRRiNu",
   "metadata": {
    "id": "0xnDmcBRRiNu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "904eaedb",
   "metadata": {},
   "source": [
    "# Run from here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5086fb1",
   "metadata": {},
   "source": [
    "Importing datasets and some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593bb65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file='IEEE_14_bus_Data_PGLib_ACOPF.xlsx'\n",
    "\n",
    "multiopf_dataset = 'Dataset_ACOPF_14bus_PGLib_t4_10admm_10sc_0.4lambda_exp5_0.1rho_10samples.npz'\n",
    "system_size = 14\n",
    "\n",
    "%run ADMM_ACOPF_Functions.ipynb \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9510033e",
   "metadata": {},
   "source": [
    "setting up GPU and RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b1822",
   "metadata": {},
   "source": [
    "create the Restoration layer\n",
    "and read the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_bus=read_data_ACOPF(File=data_file,DemFactor=1.0,print_data=False)\n",
    "\n",
    "res=create_Balance_Restoration(data_x_bus,nT=4,Ramp=False)\n",
    "\n",
    "print('wait for restoration layer construction!')\n",
    "layer_balance = CvxpyLayer(res['problem'], parameters=res['parameters'], \n",
    "                       variables=res['variables'])\n",
    "print('done!')\n",
    "\n",
    "# read the dataset again\n",
    "dataset=np.load(multiopf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b726d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9259075a",
   "metadata": {},
   "source": [
    "# Run this code for training!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35277dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "hidden_feat = 16\n",
    "\n",
    "\n",
    "NumRuns=1\n",
    "epochs=1\n",
    "\n",
    "train_loss_array=np.zeros((NumRuns,epochs+1))\n",
    "val_loss_array=np.zeros((NumRuns,epochs+1))\n",
    "Models=[]\n",
    "\n",
    "\n",
    "for run in range(NumRuns):\n",
    "\n",
    "    print(f'\\n\\n\\t=============== Run {run} =============== \\n\\n')\n",
    "    seed=run\n",
    "\n",
    "    trained_model=None\n",
    "\n",
    "\n",
    "\n",
    "    result=run_casestudy(data_x_bus,dataset,T=4,seed=seed,epochs=epochs,trained_model=trained_model)\n",
    "\n",
    "    train_loss_array[run,:]=result['training_losses']\n",
    "    val_loss_array[run,:]=result['validation_losses']\n",
    "    Models.append(result['model'])\n",
    "\n",
    "\n",
    "\n",
    "    trained_model=result['model']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28ad14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ee96fb6",
   "metadata": {},
   "source": [
    "# ML vs Test Data with Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba92061",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys=14\n",
    "Nsamples=10\n",
    "rho_off=0.01\n",
    "rho_ML=1.0\n",
    "lam_off=0.5\n",
    "lam_ML=0\n",
    "NumSc=3\n",
    "seed=0\n",
    "ADMM_iter=5\n",
    "nT=4\n",
    "\n",
    "\n",
    "TestDataSet = np.load('TestDataset_ACOPF_14bus_4t_3sc_0.01rho_0.4lambda_10samples.npz')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "r=Test_ADMMwithML(data_x_bus,TestDataSet,MLmodel=trained_model,lamda_std=lam_ML,ADMM_iter=ADMM_iter,rho=rho_ML,seed=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec5326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03c3252a",
   "metadata": {},
   "source": [
    "# ML on Test Data with no Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013fd112",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys=118\n",
    "Nsamples=50\n",
    "rho_ML=1.0\n",
    "lam_ML=0\n",
    "NumSc=1000\n",
    "seed=0\n",
    "\n",
    "\n",
    "TestDataSet = np.load(r'/content/gdrive/My Drive/GNN_Project/ADMM_Stochastic/TestDataset/TestDataset_NoSolution_118bus_0cont_1000sc_50samples.npz')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MLres=Test_ADMMwithML_NoSolution(data_x_bus,TestDataSet,MLmodel=trained_model,lamda_std=lam_ML,ADMM_iter=10,rho=rho_ML,seed=seed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9652d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "elpmNlQRYTlD",
   "metadata": {
    "id": "elpmNlQRYTlD"
   },
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
